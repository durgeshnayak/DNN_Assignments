{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64-XCLdSBBuc"
   },
   "source": [
    "# Deep Neural Networks - Programming Assignment\n",
    "## Comparing Linear Models and Multi-Layer Perceptrons\n",
    "\n",
    "**Student Name:** NAYAK DURGESH ASHOK\n",
    "**Student ID:** 2022AC05622\n",
    "\n",
    "**Student Name:** ___________________  \n",
    "**Student ID:** ___________________  \n",
    "\n",
    "**Student Name:** ___________________  \n",
    "**Student ID:** ___________________  \n",
    "\n",
    "**Student Name:** ___________________  \n",
    "**Student ID:** ___________________  \n",
    "\n",
    "**Date:** ___________________\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANT INSTRUCTIONS\n",
    "\n",
    "1. **Complete ALL sections** marked with `TODO`\n",
    "2. **DO NOT modify** the `get_assignment_results()` function structure\n",
    "3. **Track training time** for both models using `time.time()`\\n\n",
    "4. **Store loss_history** in both model classes\n",
    "5. **Calculate ALL metrics** (accuracy, precision, recall, F1)\n",
    "6. **Fill get_assignment_results()** with ALL required fields\n",
    "7. **PRINT the results** - Auto-grader needs visible output!\n",
    "8. **Run all cells** before submitting (Kernel ‚Üí Restart & Run All)\n",
    "\n",
    "**SCORING:**\n",
    "- Missing fields = 0 marks for that section\n",
    "- Non-executed notebook = 0 marks\n",
    "- Cleared outputs = 0 marks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3Pti5nJuBBue"
   },
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print('‚úì Libraries imported successfully')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNyOCAuwBBue"
   },
   "source": [
    "## Section 1: Dataset Selection and Loading\n",
    "\n",
    "**Requirements:**\n",
    "- ‚â•500 samples\n",
    "- ‚â•5 features\n",
    "- Public dataset (UCI/Kaggle)\n",
    "- Regression OR Classification problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9QfsHoIkBBuf"
   },
   "source": [
    "# TODO: Load your dataset\n",
    "# Example: data = pd.read_csv('your_dataset.csv')\n",
    "df_bcw = pd.read_csv(filepath_or_buffer='wdbc.csv')\n",
    "df_bcw.info()\n",
    "\n",
    "# Dataset information (TODO: Fill these)\n",
    "dataset_name = \"Breast Cancer Wisconsin (Diagnostic)\"  # e.g., \"Breast Cancer Wisconsin\"\n",
    "dataset_source = \"UCI Machine Learning Repository\"  # e.g., \"UCI ML Repository\"\n",
    "n_samples = 569      # Total number of rows\n",
    "n_features = 30     # Number of features (excluding target)\n",
    "problem_type = \"binary_classification\"  # \"regression\" or \"binary_classification\" or \"multiclass_classification\"\n",
    "\n",
    "# Problem statement (TODO: Write 2-3 sentences)\n",
    "problem_statement = \"We will a classifier for identifying tumors as either malignant or benign based on the features in the Breast Cancer Wisconsin dataset. Early identification of the nature of the tumor can help ensure the patient is fully informed of their condition and gets the correct treatment in the appropriate timespan. This affect their quality of life and possibly their lifespan as well.\"\n",
    "\"\"\"\n",
    "TODO: Describe what you're predicting and why it matters.\n",
    "Example: \"Predicting tumor malignancy from diagnostic measurements.\n",
    "This is critical for early cancer detection in medical diagnosis.\"\n",
    "\"\"\"\n",
    "\n",
    "# Primary evaluation metric (TODO: Fill this)\n",
    "primary_metric = \"recall\"  # e.g., \"recall\", \"accuracy\", \"rmse\", \"r2\"\n",
    "\n",
    "# Metric justification (TODO: Write 2-3 sentences)\n",
    "metric_justification = \"I chose recall because in tumor classification we do not want to have too many false negatives as this will mean patients with malignant tumors are not identified correctly.\"\n",
    "\"\"\"\n",
    "TODO: Explain why you chose this metric.\n",
    "Example: \"I chose recall because in medical diagnosis,\n",
    "false negatives (missing cancer) are more costly than false positives.\"\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Source: {dataset_source}\")\n",
    "print(f\"Samples: {n_samples}, Features: {n_features}\")\n",
    "print(f\"Problem Type: {problem_type}\")\n",
    "print(f\"Primary Metric: {primary_metric}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XOI6I6JBBuf"
   },
   "source": [
    "## Section 2: Data Preprocessing\n",
    "\n",
    "Preprocess your data:\n",
    "1. Handle missing values\n",
    "2. Encode categorical variables\n",
    "3. Split into train/test sets\n",
    "4. Scale features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mfQProeUBBuf"
   },
   "source": [
    "# TODO: Preprocess your data\n",
    "# 1. Separate features (X) and target (y)\n",
    "df_bcw['diagnosis'] = df_bcw['diagnosis'].map({'M': 1, 'B': 0})\n",
    "X = df_bcw.drop('diagnosis', axis=1)\n",
    "y = df_bcw['diagnosis']\n",
    "# 2. Handle missing values if any\n",
    "# Dataset does not have missing data\n",
    "# 3. Encode categorical variables\n",
    "# Done above before separating features and target\n",
    "# Example:\n",
    "# X = data.drop('target', axis=1)\n",
    "# y = data['target']\n",
    "\n",
    "# TODO: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fill these after preprocessing\n",
    "train_samples = X_train.shape[0]       # Number of training samples\n",
    "test_samples = X_test.shape[0]        # Number of test samples\n",
    "train_test_ratio = (train_samples / X.shape[0])  # e.g., 0.8 for 80-20 split\n",
    "\n",
    "print(f\"Train samples: {train_samples}\")\n",
    "print(f\"Test samples: {test_samples}\")\n",
    "print(f\"Split ratio: {train_test_ratio:.1%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Elw6-stdBBuf"
   },
   "source": [
    "|## Section 3: Baseline Model Implementation\n",
    "\n",
    "Implement from scratch (NO sklearn models!):\n",
    "- Linear Regression (for regression)\n",
    "- Logistic Regression (for binary classification)\n",
    "- Softmax Regression (for multiclass classification)\n",
    "\n",
    "**Must include:**\n",
    "- Forward pass (prediction)\n",
    "- Loss computation\n",
    "- Gradient computation\n",
    "- Gradient descent loop\n",
    "- Loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bvQ2rWfWBBuf"
   },
   "source": [
    "class BaselineModel:\n",
    "    \"\"\"\n",
    "    Baseline linear model with gradient descent\n",
    "    Implement: Linear/Logistic/Softmax Regression\n",
    "    Single Neuron Binary Classifier - Activation Function (Sigmoid) - Objective Function (Binary Cross Entropy Loss)\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\" Sigmoid Activation Function Implementation \"\"\"\n",
    "        clipped_ws = np.clip(z, -500, 500)\n",
    "        sig = 1 / (1 + np.exp(-clipped_ws))\n",
    "        return sig\n",
    "\n",
    "    # Step 0\n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\" Initialize weights with small random numbers and bias to zero \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.randn(n_features) * 0.01\n",
    "        self.bias = 0.0\n",
    "\n",
    "    # Step 1\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\" Forward pass to evaluate the predictions \"\"\"\n",
    "        # Weighted Summation\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "        # Sigmoid Activation \n",
    "        y_pred = self.sigmoid(z)\n",
    "        return y_pred\n",
    "\n",
    "    # Step 2\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\" Binary cross-entropy loss\n",
    "                Loss = -1/N * sum(y*log(y_pred) + (1-y)*log(1-y_pred))\n",
    "        \"\"\"\n",
    "        # Small correct to prevent log(0)\n",
    "        correction = 1e-15\n",
    "        #y_pred = np.clip(y_pred, correction, 1-correction)\n",
    "\n",
    "        bce_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return bce_loss\n",
    "\n",
    "    # Step 3\n",
    "    def calculate_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\" Calculate gradients for weights and bias\n",
    "                dL/dw = 1/N * X^T * (y_pred - y_true)\n",
    "                dL/db = 1/N * sum(y_pred - y_true)\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        dl_dw = (1 / N) * (np.dot(X.T, error))\n",
    "        dl_db = (1 / N) * sum(error)\n",
    "\n",
    "        return dl_dw, dl_db\n",
    "\n",
    "    # Step 4\n",
    "    def update_parameters(self, dl_dw, dl_db):\n",
    "        \"\"\" Update weights and bias simulateneously according to SGD formula\n",
    "                new_w = w - (learning_rate * dl_dw)\n",
    "                new_b = b - (learning_rate * dl_db)\n",
    "        \"\"\"\n",
    "        self.weights -= self.lr * dl_dw\n",
    "        self.bias -= self.lr * dl_db\n",
    "\n",
    "    # Step 5\n",
    "    def calculate_accuracy(self, y_true, y_pred, threshold):\n",
    "        \"\"\" Calculate accuracy of the classification \"\"\"\n",
    "        y_class_pred = (y_pred >= threshold).astype(int)\n",
    "        accuracy = np.mean(y_class_pred == y_true)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO: Implement gradient descent training\n",
    "\n",
    "        Steps:\n",
    "        1. Initialize weights and bias\n",
    "        2. For each iteration:\n",
    "           a. Compute predictions (forward pass)\n",
    "           b. Compute loss\n",
    "           c. Compute gradients\n",
    "           d. Update weights and bias\n",
    "           e. Store loss in self.loss_history\n",
    "\n",
    "        Must populate self.loss_history with loss at each iteration!\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # TODO: Initialize parameters\n",
    "        self.initialize_parameters(n_features=n_features)\n",
    "\n",
    "        print('Commencing training ---------------------------->')\n",
    "        print('Hyperparameters:')\n",
    "        print(f'Learning Rate: {self.lr}')\n",
    "        print(f'Trainings Iterations: {self.n_iterations}')\n",
    "        print(f'Number of training samples: {X.shape[0]}')\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # 1. Forward Pass\n",
    "            y_pred = self.forward_pass(X=X)\n",
    "\n",
    "            # 2. Compute loss\n",
    "            loss = self.compute_loss(y_true=y, y_pred=y_pred)\n",
    "\n",
    "            # 3. Compute accuracy\n",
    "            accuracy = self.calculate_accuracy(y_true=y, y_pred=y_pred, threshold=0.5)\n",
    "\n",
    "            # 4. Compute gradients\n",
    "            dl_dw, dl_db = self.calculate_gradients(X=X, y_true=y, y_pred=y_pred)\n",
    "\n",
    "            # 5. Update parameters\n",
    "            self.update_parameters(dl_dw=dl_dw, dl_db=dl_db)\n",
    "\n",
    "            # 6. Store Metrics\n",
    "            self.loss_history.append(loss)\n",
    "            self.accuracy_history.append(loss)\n",
    "\n",
    "        # TODO: Implement gradient descent loop\n",
    "        #for i in range(self.n_iterations):\n",
    "            # 1. Forward pass: y_pred = ...\n",
    "            # 2. Compute loss\n",
    "            # 3. Compute gradients: dw = ..., db = ...\n",
    "            # 4. Update: self.weights -= self.lr * dw\n",
    "            # 5. self.loss_history.append(loss)\n",
    "            #pass  # Replace with your implementation\n",
    "            \n",
    "\n",
    "        print('Training completed ---------------------------->')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Implement prediction\n",
    "\n",
    "        For regression: return linear_output\n",
    "        For classification: return class probabilities or labels\n",
    "        \"\"\"\n",
    "        #pass  # Replace with your implementation\n",
    "        return self.forward_pass(X)\n",
    "\n",
    "print(\"‚úì Baseline model class defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "grpLqkmTBBuf"
   },
   "source": [
    "# Train baseline model\n",
    "print(\"Training baseline model...\")\n",
    "baseline_start = time.time()\n",
    "\n",
    "# TODO: Initialize and train your baseline model\n",
    "baseline_model = BaselineModel(learning_rate=0.1, n_iterations=1000)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# TODO: Make predictions\n",
    "baseline_predictions = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "baseline_training_time = time.time() - baseline_start\n",
    "print(f\"‚úì Baseline training completed in {baseline_training_time:.2f}s\")\n",
    "print(f\"‚úì Loss decreased from {baseline_model.loss_history[0]:.4f} to {baseline_model.loss_history[-1]:.4f}\")\n",
    "\n",
    "# Store loss explicitly\n",
    "baseline_initial_loss = baseline_model.loss_history[0]\n",
    "baseline_final_loss = baseline_model.loss_history[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQpy-zZSBBug"
   },
   "source": [
    "## Section 4: Multi-Layer Perceptron Implementation\n",
    "\n",
    "Implement MLP from scratch with:\n",
    "- At least 1 hidden layer\n",
    "- ReLU activation for hidden layers\n",
    "- Appropriate output activation\n",
    "- Forward propagation\n",
    "- Backward propagation\n",
    "- Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r2wqZVlfBBug"
   },
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron implemented from scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, architecture, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        architecture: list [input_size, hidden1, hidden2, ..., output_size]\n",
    "        Example: [30, 16, 8, 1] means:\n",
    "            - 30 input features\n",
    "            - Hidden layer 1: 16 neurons\n",
    "            - Hidden layer 2: 8 neurons\n",
    "            - Output layer: 1 neuron\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.parameters = {}\n",
    "        self.loss_history = []\n",
    "        self.cache = {}\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        TODO: Initialize weights and biases for all layers\n",
    "\n",
    "        For each layer l:\n",
    "        - W[l]: weight matrix of shape (n[l], n[l-1])\n",
    "        - b[l]: bias vector of shape (n[l], 1)\n",
    "\n",
    "        Store in self.parameters dictionary\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for l in range(1, len(self.architecture)):\n",
    "            # TODO: Initialize weights and biases\n",
    "            # self.parameters[f'W{l}'] = ...\n",
    "            # self.parameters[f'b{l}'] = ...\n",
    "            pass\n",
    "\n",
    "    def relu(self, Z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"ReLU derivative\"\"\"\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid activation (for binary classification output)\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Implement forward pass through all layers\n",
    "\n",
    "        For each layer:\n",
    "        1. Z[l] = W[l] @ A[l-1] + b[l]\n",
    "        2. A[l] = activation(Z[l])\n",
    "\n",
    "        Store Z and A in self.cache for backpropagation\n",
    "        Return final activation A[L]\n",
    "        \"\"\"\n",
    "        self.cache['A0'] = X\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        # for l in range(1, len(self.architecture)):\n",
    "        #     ...\n",
    "\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "    def backward_propagation(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO: Implement backward pass to compute gradients\n",
    "\n",
    "        Starting from output layer, compute:\n",
    "        1. dZ[l] for each layer\n",
    "        2. dW[l] = dZ[l] @ A[l-1].T / m\n",
    "        3. db[l] = sum(dZ[l]) / m\n",
    "\n",
    "        Return dictionary of gradients\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        grads = {}\n",
    "\n",
    "        # TODO: Implement backward pass\n",
    "        # Start with output layer gradient\n",
    "        # Then propagate backwards through hidden layers\n",
    "\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\"\n",
    "        TODO: Update weights and biases using gradients\n",
    "\n",
    "        For each layer:\n",
    "        W[l] = W[l] - learning_rate * dW[l]\n",
    "        b[l] = b[l] - learning_rate * db[l]\n",
    "        \"\"\"\n",
    "        # TODO: Implement parameter updates\n",
    "        pass\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        TODO: Compute loss\n",
    "\n",
    "        For regression: MSE\n",
    "        For classification: Cross-entropy\n",
    "        \"\"\"\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO: Implement training loop\n",
    "\n",
    "        For each iteration:\n",
    "        1. Forward propagation\n",
    "        2. Compute loss\n",
    "        3. Backward propagation\n",
    "        4. Update parameters\n",
    "        5. Store loss\n",
    "\n",
    "        Must populate self.loss_history!\n",
    "        \"\"\"\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # TODO: Training loop\n",
    "            pass\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Implement prediction\n",
    "\n",
    "        Use forward_propagation and apply appropriate thresholding\n",
    "        \"\"\"\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "print(\"‚úì MLP class defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "foCVrsiTBBug"
   },
   "source": [
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "mlp_start_time = time.time()\n",
    "\n",
    "# TODO: Define your architecture and train MLP\n",
    "mlp_architecture = []  # Example: [n_features, 16, 8, 1]\n",
    "mlp_model = MLP(architecture=mlp_architecture, learning_rate=0.01, n_iterations=1000)\n",
    "# mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# TODO: Make predictions\n",
    "# mlp_predictions = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "mlp_training_time = time.time() - mlp_start_time\n",
    "print(f\"‚úì MLP training completed in {mlp_training_time:.2f}s\")\n",
    "print(f\"‚úì Loss decreased from {mlp_model.loss_history[0]:.4f} to {mlp_model.loss_history[-1]:.4f}\")\n",
    "\n",
    "# Store loss explicitly\n",
    "mlp_initial_loss = 0.0 #mlp_model.loss_history[0]\n",
    "mlp_final_loss = 0.0 #mlp_model.loss_history[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf38UvC0BBug"
   },
   "source": [
    "## Section 5: Evaluation and Metrics\n",
    "\n",
    "Calculate appropriate metrics for your problem type"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dei5PjKGBBug"
   },
   "source": [
    "def calculate_metrics(y_true, y_pred, problem_type):\n",
    "    \"\"\"\n",
    "    TODO: Calculate appropriate metrics based on problem type\n",
    "\n",
    "    For regression: MSE, RMSE, MAE, R¬≤\n",
    "    For classification: Accuracy, Precision, Recall, F1\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    if problem_type == \"regression\":\n",
    "        # TODO: Calculate regression metrics\n",
    "        # TODO: Implement from scratch\n",
    "        mse = 0.0\n",
    "        rmse = 0.0\n",
    "        mae = 0.0\n",
    "        r2 = 0.0\n",
    "        return mse, rmse, mae, r2\n",
    "        pass\n",
    "    elif problem_type in [\"binary_classification\", \"multiclass_classification\"]:\n",
    "        # TODO: Calculate classification metrics\n",
    "        # TODO: Implement from scratch (no sklearn.metrics)\n",
    "        accuracy = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        f1 = 0.0\n",
    "        return accuracy, precision, recall, f1\n",
    "        pass\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for both models\n",
    "# baseline_metrics = calculate_metrics(y_test, baseline_predictions, problem_type)\n",
    "# mlp_metrics = calculate_metrics(y_test, mlp_predictions, problem_type)\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "# print(baseline_metrics)\n",
    "\n",
    "print(\"\\nMLP Model Performance:\")\n",
    "# print(mlp_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4SZCkf3BBug"
   },
   "source": [
    "## Section 6: Visualization\n",
    "\n",
    "Create visualizations:\n",
    "1. Training loss curves\n",
    "2. Performance comparison\n",
    "3. Additional domain-specific plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hPqji5F0BBuh"
   },
   "source": [
    "# 1. Training loss curves\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Plot baseline loss\n",
    "# plt.plot(baseline_model.loss_history, label='Baseline', color='blue')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Baseline Model - Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# TODO: Plot MLP loss\n",
    "# plt.plot(mlp_model.loss_history, label='MLP', color='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP Model - Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r6Pj6pQZBBuh"
   },
   "source": [
    "# 2. Performance comparison bar chart\n",
    "# TODO: Create bar chart comparing key metrics between models\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Example:\n",
    "# metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "# baseline_scores = [baseline_metrics[m] for m in metrics]\n",
    "# mlp_scores = [mlp_metrics[m] for m in metrics]\n",
    "#\n",
    "# x = np.arange(len(metrics))\n",
    "# width = 0.35\n",
    "#\n",
    "# plt.bar(x - width/2, baseline_scores, width, label='Baseline')\n",
    "# plt.bar(x + width/2, mlp_scores, width, label='MLP')\n",
    "# plt.xlabel('Metrics')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Model Performance Comparison')\n",
    "# plt.xticks(x, metrics)\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua107rhpBBuh"
   },
   "source": [
    "## Section 7: Analysis and Discussion\n",
    "\n",
    "Write your analysis (minimum 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0fdNLJqCBBuh"
   },
   "source": [
    "analysis_text = \"\"\"\n",
    "TODO: Write your analysis here (minimum 200 words)\n",
    "\n",
    "Address these questions:\n",
    "1. Which model performed better and by how much?\n",
    "2. Why do you think one model outperformed the other?\n",
    "3. What was the computational cost difference (training time)?\n",
    "4. Any surprising findings or challenges you faced?\n",
    "5. What insights did you gain about neural networks vs linear models?\n",
    "\n",
    "Write your thoughtful analysis here. Be specific and reference your actual results.\n",
    "Compare the metrics, discuss the trade-offs, and explain what you learned.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
    "if len(analysis_text.split()) < 200:\n",
    "    print(\"‚ö†Ô∏è  Warning: Analysis should be at least 200 words\")\n",
    "else:\n",
    "    print(\"‚úì Analysis meets word count requirement\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e3C2Bf4BBuh"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## ‚≠ê REQUIRED: Structured Output Function\n",
    "\n",
    "### **DO NOT MODIFY THE STRUCTURE BELOW**\n",
    "\n",
    "This function will be called by the auto-grader. Fill in all values accurately based on your actual results.\n",
    "\n",
    "\n",
    "‚≠ê‚≠ê‚≠ê REQUIRED: Structured Output Function ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "### üö® CRITICAL - READ CAREFULLY üö®\n",
    "\n",
    "1. **Fill in ALL fields** - Missing fields = 0 marks\n",
    "2. **Use your actual values** - Not 0 or empty strings\n",
    "3. **This cell MUST be executed** - We need the output!\n",
    "4. **Print the results** - Auto-grader needs to see output!\n",
    "\n",
    "\n",
    "**DO NOT:**\n",
    "- Leave any field as 0, 0.0,\n",
    "- Clear outputs before submission\n",
    "- Modify the structure\n",
    "\n",
    "\n",
    "\"**MUST DO:**\n",
    "- Fill every field with your actual results\n",
    "- Execute this cell and keep the output\n",
    "- Print the results (see below)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w1W6qvYzBBuh"
   },
   "source": [
    "def get_assignment_results():\n",
    "    '''\n",
    "    CRITICAL: Fill ALL fields with your actual results!\n",
    "    Missing fields will result in 0 marks for that section.\n",
    "    '''\n",
    "\n",
    "    results = {\n",
    "        # ===== Dataset Information (1 mark) =====\n",
    "        'dataset_name': dataset_name,  # MUST fill\n",
    "        'dataset_source': dataset_source,  # MUST fill\n",
    "        'n_samples': n_samples,  # MUST be ‚â•500\n",
    "        'n_features': n_features,  # MUST be ‚â•5\n",
    "        'problem_type': problem_type,  # MUST fill\n",
    "        'problem_statement': problem_statement,  # MUST be ‚â•50 words\n",
    "        'primary_metric': primary_metric,  # MUST fill\n",
    "        'metric_justification': metric_justification,  # MUST be ‚â•30 words\n",
    "        'train_samples': train_samples,\n",
    "        'test_samples': test_samples,\n",
    "        'train_test_ratio': train_test_ratio,\n",
    "\n",
    "        # ===== Baseline Model (3 marks) =====\n",
    "        'baseline_model': {\n",
    "            'model_type': '',  # 'linear_regression', 'logistic_regression', 'softmax_regression'\n",
    "            'learning_rate': 0.01,  # Your learning rate\n",
    "            'n_iterations': 1000,  # Your iterations\n",
    "\n",
    "            # CRITICAL: These MUST be filled!\n",
    "            'initial_loss': baseline_initial_loss,  # MUST NOT be 0\n",
    "            'final_loss': baseline_final_loss,  # MUST NOT be 0\n",
    "            'training_time_seconds': baseline_training_time,  # MUST NOT be 0\n",
    "            'loss_decreased': baseline_final_loss < baseline_initial_loss,  # Auto-calculated\n",
    "\n",
    "            # Metrics - Fill based on your problem type\n",
    "            'test_accuracy': 0.0 if problem_type == 'regression' else baseline_acc,\n",
    "            'test_precision': 0.0 if problem_type == 'regression' else baseline_prec,\n",
    "            'test_recall': 0.0 if problem_type == 'regression' else baseline_rec,\n",
    "            'test_f1': 0.0 if problem_type == 'regression' else baseline_f1,\n",
    "            'test_mse': baseline_mse if problem_type == 'regression' else 0.0,\n",
    "            'test_rmse': baseline_rmse if problem_type == 'regression' else 0.0,\n",
    "            'test_mae': baseline_mae if problem_type == 'regression' else 0.0,\n",
    "            'test_r2': baseline_r2 if problem_type == 'regression' else 0.0,\n",
    "        },\n",
    "\n",
    "        # ===== MLP Model (4 marks) =====\n",
    "        'mlp_model': {\n",
    "            'architecture': mlp_architecture,  # MUST have ‚â•3 elements\n",
    "            'n_hidden_layers': len(mlp_architecture) - 2 if len(mlp_architecture) > 0 else 0,\n",
    "            'learning_rate': 0.01,\n",
    "            'n_iterations': 1000,\n",
    "\n",
    "            # CRITICAL: These MUST be filled!\n",
    "            'initial_loss': mlp_initial_loss,  # MUST NOT be 0\n",
    "            'final_loss': mlp_final_loss,  # MUST NOT be 0\n",
    "            'training_time_seconds': mlp_training_time,  # MUST NOT be 0\n",
    "            'loss_decreased': mlp_final_loss < mlp_initial_loss,  # Auto-calculated\n",
    "\n",
    "            # Metrics\n",
    "            'test_accuracy': 0.0 if problem_type == 'regression' else mlp_acc,\n",
    "            'test_precision': 0.0 if problem_type == 'regression' else mlp_prec,\n",
    "            'test_recall': 0.0 if problem_type == 'regression' else mlp_rec,\n",
    "            'test_f1': 0.0 if problem_type == 'regression' else mlp_f1,\n",
    "            'test_mse': mlp_mse if problem_type == 'regression' else 0.0,\n",
    "            'test_rmse': mlp_rmse if problem_type == 'regression' else 0.0,\n",
    "            'test_mae': mlp_mae if problem_type == 'regression' else 0.0,\n",
    "            'test_r2': mlp_r2 if problem_type == 'regression' else 0.0,\n",
    "        },\n",
    "\n",
    "        # ===== Analysis (2 marks) =====\n",
    "        'analysis': analysis_text,\n",
    "        'analysis_word_count': len(analysis_text.split()),\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# ===== CRITICAL: CALL AND PRINT RESULTS =====\n",
    "# This MUST be executed and output MUST be visible!\n",
    "import json\n",
    "results = get_assignment_results()\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# ===== Validation =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "errors = []\n",
    "\n",
    "if results['n_samples'] < 500:\n",
    "    errors.append(f\"‚ùå Dataset too small: {results['n_samples']} < 500\")\n",
    "if results['n_features'] < 5:\n",
    "    errors.append(f\"‚ùå Too few features: {results['n_features']} < 5\")\n",
    "if results['baseline_model']['initial_loss'] == 0:\n",
    "    errors.append(\"‚ùå Baseline initial_loss is 0\")\n",
    "if results['baseline_model']['final_loss'] == 0:\n",
    "    errors.append(\"‚ùå Baseline final_loss is 0\")\n",
    "if results['baseline_model']['training_time_seconds'] == 0:\n",
    "    errors.append(\"‚ùå Baseline training_time is 0\")\n",
    "if results['mlp_model']['initial_loss'] == 0:\n",
    "    errors.append(\"‚ùå MLP initial_loss is 0\")\n",
    "if results['mlp_model']['final_loss'] == 0:\n",
    "    errors.append(\"‚ùå MLP final_loss is 0\")\n",
    "if results['mlp_model']['training_time_seconds'] == 0:\n",
    "    errors.append(\"‚ùå MLP training_time is 0\")\n",
    "if len(results['mlp_model']['architecture']) < 3:\n",
    "    errors.append(\"‚ùå MLP architecture invalid\")\n",
    "if results['analysis_word_count'] < 200:\n",
    "    errors.append(f\"‚ùå Analysis too short: {results['analysis_word_count']} < 200 words\")\n",
    "\n",
    "if errors:\n",
    "    print(\"ERRORS FOUND:\")\n",
    "    for err in errors:\n",
    "        print(err)\n",
    "    print(\" FIX THESE BEFORE SUBMITTING! \")\n",
    "else:\n",
    "    print(\"‚úÖ All validation checks passed!\")\n",
    "    print(\"‚úÖ Ready to submit!\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Kernel ‚Üí Restart & Clear Output\")\n",
    "    print(\"2. Kernel ‚Üí Restart & Run All\")\n",
    "    print(\"3. Verify this output is visible\")\n",
    "    print(\"4. Save notebook\")\n",
    "    print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
    "    print(\"6. Submit to LMS\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPvwotBWBBuh"
   },
   "source": [
    "## Test Your Output\n",
    "\n",
    "Run this cell to verify your results dictionary is complete and properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n4mkH-nSBBui"
   },
   "source": [
    "# Test the output\n",
    "import json\n",
    "\n",
    "try:\n",
    "    results = get_assignment_results()\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(json.dumps(results, indent=2))\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = []\n",
    "    def check_dict(d, prefix=\"\"):\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                check_dict(v, f\"{prefix}{k}.\")\n",
    "            elif (v == 0 or v == \"\" or v == 0.0 or v == []) and \\\n",
    "                 k not in ['improvement', 'improvement_percentage', 'baseline_better',\n",
    "                          'baseline_converged', 'mlp_converged', 'total_parameters',\n",
    "                          'test_accuracy', 'test_precision', 'test_recall', 'test_f1',\n",
    "                          'test_mse', 'test_rmse', 'test_mae', 'test_r2']:\n",
    "                missing.append(f\"{prefix}{k}\")\n",
    "\n",
    "    check_dict(results)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(missing)} fields still need to be filled:\")\n",
    "        for m in missing[:15]:  # Show first 15\n",
    "            print(f\"  - {m}\")\n",
    "        if len(missing) > 15:\n",
    "            print(f\"  ... and {len(missing)-15} more\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required fields are filled!\")\n",
    "        print(\"\\nüéâ You're ready to submit!\")\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Kernel ‚Üí Restart & Clear Output\")\n",
    "        print(\"2. Kernel ‚Üí Restart & Run All\")\n",
    "        print(\"3. Verify no errors\")\n",
    "        print(\"4. Save notebook\")\n",
    "        print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
    "        print(\"6. Submit to LMS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in get_assignment_results(): {str(e)}\")\n",
    "    print(\"\\nPlease fix the errors above before submitting.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuRKuL80BBui"
   },
   "source": [
    "---\n",
    "\n",
    "## üì§ Before Submitting - Final Checklist\n",
    "\n",
    "- [ ] **All TODO sections completed**\n",
    "- [ ] **Both models implemented from scratch** (no sklearn models!)\n",
    "- [ ] **get_assignment_results() function filled accurately**\n",
    "- [ ] **Loss decreases for both models**\n",
    "- [ ] **Analysis ‚â• 200 words**\n",
    "- [ ] **All cells run without errors** (Restart & Run All)\n",
    "- [ ] **Visualizations created**\n",
    "- [ ] **File renamed correctly**: YourStudentID_assignment.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck! **"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
